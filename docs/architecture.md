# Техническая документация: Архитектура системы Q&A по документам СРО НОСО

## 1. Обзор архитектуры

Система построена по архитектуре Retrieval-Augmented Generation (RAG) и состоит из следующих уровней:

### 1.1 Уровень данных
- Хранение оригинальных документов в папке `documents/`
- Векторная база данных ChromaDB в `data/chroma_db/`
- Метаданные в JSON-файлах (например, `progress_metadata_hierarchical.json`)

### 1.2 Уровень обработки
- Модуль `chunking.py`: Разбиение документов на семантически связанные чанки
- Модуль `embed_store.py`: Создание эмбеддингов с моделью intfloat/multilingual-e5-base
- Модуль `add_metadata.py`: Генерация метаданных с qwen2.5:3b в Ollama

### 1.3 Уровень поиска
- Модуль `retrieval.py`: Векторный поиск и ранжирование результатов

### 1.4 Уровень генерации
- Интеграция с DeepSeek (deepseek-chat) через OpenAI API для генерации ответов

### 1.5 Уровень пользовательского интерфейса
- Основной файл `rag_app.py`: Gradio-интерфейс с вкладками чата, поиска и администрирования

## 2. Ключевые модули

### 2.1 rag_app.py
- Главный файл приложения
- Создает вкладки Gradio: чат с AI, поиск документов, администрирование
- Интегрирует все модули для взаимодействия

### 2.2 src/chunking.py
- Функция `process_text_with_hierarchy()`: Чанкирование с учетом иерархии заголовков
- Возвращает список чанков с метаданными структуры

### 2.3 src/embed_store.py
- Класс `EmbeddingStore`: Управление коллекциями ChromaDB
- Методы: `add_documents()`, `get_or_create_collection()`, батчевая обработка эмбеддингов

### 2.4 src/retrieval.py
- Функция `search_and_rank()`: Векторный поиск топ-5 чанков
- Ранжирование по косинусному сходству

### 2.5 src/add_metadata.py
- Функция `add_metadata_with_llm()`: Аннотация чанков с qwen2.5:3b
- Генерация резюме, ключевых слов, категории, вопросов

### 2.6 src/check_integrity.py
- Валидация документов и баз данных

## 3. Поток данных

1. Документы загружаются из `documents/txts/`
2. `chunking.py` разбивает текст на чанки
3. `add_metadata.py` добавляет метаданные
4. `embed_store.py` создает эмбеддинги и сохраняет в ChromaDB
5. При запросе: `retrieval.py` ищет релевантные чанки
6. `rag_app.py` генерирует ответ с DeepSeek и отображает в Gradio

## 4. Зависимости

Основные библиотеки (установить с `pip install`):
- sentence-transformers>=2.0.0
- chromadb>=0.4.0
- langchain>=0.1.0
- openai>=1.0.0
- gradio>=4.0.0
- python-dotenv>=1.0.0
- ollama>=0.2.0 (для локального LLM)

Для обработки документов:
- python-docx (чтение DOCX)
- PyPDF2 (чтение PDF)

Конфигурация в `.env`:
- DEEPSEEK_API_KEY=ваш_ключ
- OLLAMA_BASE_URL=http://localhost:11434 (по умолчанию)

## 5. API интеграции

- DeepSeek API: https://platform.deepseek.com (клиент через openai)
- Ollama API: локальный сервер для qwen2.5:3b
- ChromaDB: встраиваемый векторный поиск

## 6. Производительность и лимиты

- Максимум 1000 токенов на ответ
- Температура 0.3 для детерминированности
- Батч эмбеддингов: 100 чанков
- Поддержка до 100 документов, 500к чанков
